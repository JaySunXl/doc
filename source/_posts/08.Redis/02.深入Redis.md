---
title: 深入Redis
date: 2018-03-16
tags:
- Redis
- Cache
---

<!-- TOC -->

- [速度快的原因](#速度快的原因)
- [二八定律](#二八定律)
- [概念](#概念)
    - [缓存雪崩](#缓存雪崩)
    - [缓存穿透](#缓存穿透)
    - [缓存预热](#缓存预热)
    - [缓存更新](#缓存更新)
- [缓存淘汰](#缓存淘汰)

<!-- /TOC -->

# 速度快的原因

* 基于内存存储
* 单线程,避免了不必要的上下文切换和各种锁问题
* 多路I/O复用模型，非阻塞IO(多个连接复用一个线程)

> 基于内存操作,因此CPU不会成为影响性能的瓶颈,从而在单线程下足够快,没有必要采取多线程的方案

# 二八定律




# 概念

## 缓存雪崩

场景:

对一批缓存设置了同样的过期时间,在过期后的同一时间,原本能访问到缓存的请求都去访问数据库,给数据库造成巨大压力,继而出现严重的连锁问题.

解决雪崩问题的关键在于避免大量的请求同时访问数据库,在上面的场景中,缓存同时失效是问题的诱因,因此合理分配缓存的过期时间即可有效避免雪崩,
即使无法避免,也可以采用对请求排队的方式来减轻服务器的压力,但是这种解决方案会造成吞吐量的下降,不推荐在高并发场景下使用.
还有一种治本的方案是对缓存设置好过期标志,监测到过期即重建缓存.

## 缓存穿透

场景:

某一用户根本不存在,在查询时,现在缓存中查找,然后又去数据库查找.白白浪费了两次查找的时间.

对空结果进行缓存,那么下一次查询会再缓存中查到空,而不会再去查询数据库
使用布隆过滤器,将所有可能存在于缓存中的数据哈希到bitmap中,缓存中没有同时不存在于此bitmap中的则不去查询数据库

## 缓存预热

场景:

系统上线后就创建好缓存,避免初次请求的长时间响应

## 缓存更新

Redis中默认有6种缓存失效策略,除此之外,常见自定义的缓存策略

定时清理过期缓存
当有请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存


# 缓存淘汰

可以设置内存最大使用量,当使用量超出时,会根据数据淘汰策略移出数据.

支持的6种缓存淘汰策略:

| 策略            | 说明                               |
| --------------- | ---------------------------------- |
| volatile-lru    | 淘汰最近最少使用的数据             |
| volatile-ttl    | 即将要过期的数据                   |
| volatile-random | 任意选择数据淘汰                   |
| allkeys-lru     | 所有数据中最近最少使用的数据被淘汰 |
| allkeys-random  | 所有数据中随机选择数据淘汰         |
| no-enviction    | 不淘汰任何数据,但不可再写入        |

> volatile-*的策略都是针对配置了过期时间的缓存
> allkeys-*的是针对所有数据集
> LRU:最近最少使用
> random:随机


手写一个LRU算法:
```Java
public class LRUCache<K, V> extends LinkedHashMap<K, V> {
    private       int size = 0;
    private final int MAX_SIZE;

    public LRUCache(int size) {
        super(size, 0.75f, true);
        MAX_SIZE = size;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
        return size() > MAX_SIZE;
    }
}
```

[![](https://static.segmentfault.com/v-5b1df2a7/global/img/creativecommons-cc.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/)